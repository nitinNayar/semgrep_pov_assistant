"""
File utility functions for semgrep_pov_assistant.

This module provides functions for handling file operations, including
reading transcript files in various formats, managing file paths,
and handling file encoding detection.
"""

import os
import chardet
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Union
import logging

from .logger import get_logger

logger = get_logger()

class FileUtils:
    """Utility class for file operations."""
    
    # Supported file extensions
    SUPPORTED_EXTENSIONS = {'.txt', '.docx', '.pdf', '.md', '.py'}
    
    # Common transcript file patterns
    TRANSCRIPT_PATTERNS = [
        '*transcript*',
        '*call*',
        '*meeting*',
        '*conversation*'
    ]
    
    @staticmethod
    def detect_encoding(file_path: Union[str, Path]) -> str:
        """
        Detect the encoding of a text file.
        
        Args:
            file_path: Path to the file
            
        Returns:
            Detected encoding (e.g., 'utf-8', 'iso-8859-1')
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        # Read a sample of the file to detect encoding
        with open(file_path, 'rb') as f:
            raw_data = f.read(10000)  # Read first 10KB
        
        result = chardet.detect(raw_data)
        encoding = result['encoding']
        confidence = result['confidence']
        
        logger.debug(f"Detected encoding: {encoding} (confidence: {confidence:.2f})")
        
        return encoding or 'utf-8'  # Default to UTF-8 if detection fails
    
    @staticmethod
    def read_text_file(file_path: Union[str, Path], encoding: Optional[str] = None) -> str:
        """
        Read a text file with proper encoding detection.
        
        Args:
            file_path: Path to the text file
            encoding: Optional encoding to use (if None, will auto-detect)
            
        Returns:
            File contents as string
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        # Detect encoding if not provided
        if encoding is None:
            encoding = FileUtils.detect_encoding(file_path)
        
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                content = f.read()
            logger.debug(f"Successfully read file: {file_path}")
            return content
        except UnicodeDecodeError as e:
            logger.warning(f"Failed to read with encoding {encoding}, trying UTF-8: {e}")
            # Fallback to UTF-8
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return content
    
    @staticmethod
    def get_supported_files(directory: Union[str, Path]) -> List[Path]:
        """
        Get all supported transcript files from a directory.
        
        Args:
            directory: Directory to search for files
            
        Returns:
            List of Path objects for supported files
        """
        directory = Path(directory)
        
        if not directory.exists():
            logger.warning(f"Directory does not exist: {directory}")
            return []
        
        supported_files = []
        
        for file_path in directory.rglob('*'):
            if file_path.is_file() and file_path.suffix.lower() in FileUtils.SUPPORTED_EXTENSIONS:
                # Check if filename matches transcript patterns
                filename_lower = file_path.name.lower()
                if any(pattern.replace('*', '') in filename_lower for pattern in FileUtils.TRANSCRIPT_PATTERNS):
                    supported_files.append(file_path)
        
        logger.info(f"Found {len(supported_files)} supported transcript files in {directory}")
        return supported_files
    
    @staticmethod
    def clean_text_content(text: str) -> str:
        """
        Clean and normalize text content from transcript files.
        
        Args:
            text: Raw text content
            
        Returns:
            Cleaned text content
        """
        if not text:
            return ""
        
        # Remove common transcript artifacts
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            # Skip empty lines
            if not line.strip():
                continue
            
            # Skip common transcript artifacts
            line_lower = line.lower().strip()
            if any(skip_pattern in line_lower for skip_pattern in [
                'transcript', 'generated by', 'auto-generated',
                'speaker', 'participant', 'time:', 'duration:'
            ]):
                continue
            
            # Clean the line
            cleaned_line = line.strip()
            if cleaned_line:
                cleaned_lines.append(cleaned_line)
        
        # Join lines and normalize whitespace
        cleaned_text = '\n'.join(cleaned_lines)
        
        # Remove excessive whitespace
        import re
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
        cleaned_text = re.sub(r'\n\s*\n', '\n\n', cleaned_text)
        
        return cleaned_text.strip()
    
    @staticmethod
    def extract_metadata_from_filename(file_path: Path) -> Dict[str, str]:
        """
        Extract metadata from filename (date, participants, etc.).
        
        Args:
            file_path: Path to the transcript file
            
        Returns:
            Dictionary with extracted metadata
        """
        filename = file_path.stem.lower()
        
        metadata = {
            'filename': file_path.name,
            'file_path': str(file_path),
            'file_size': file_path.stat().st_size,
            'modified_date': file_path.stat().st_mtime
        }
        
        # Try to extract date from filename
        import re
        date_patterns = [
            r'(\d{4}-\d{2}-\d{2})',  # YYYY-MM-DD
            r'(\d{2}-\d{2}-\d{4})',  # MM-DD-YYYY
            r'(\d{8})',               # YYYYMMDD
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, filename)
            if match:
                metadata['extracted_date'] = match.group(1)
                break
        
        # Try to extract call type
        call_types = ['discovery', 'demo', 'follow-up', 'pov', 'kickoff', 'review']
        for call_type in call_types:
            if call_type in filename:
                metadata['call_type'] = call_type
                break
        
        return metadata
    
    @staticmethod
    def create_output_directory(output_dir: Union[str, Path]) -> Path:
        """
        Create output directory if it doesn't exist.
        
        Args:
            output_dir: Path to the output directory
            
        Returns:
            Path object for the created directory
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        logger.info(f"Output directory ready: {output_path}")
        return output_path
    
    @staticmethod
    def save_intermediate_result(data: Dict, filename: str, output_dir: Union[str, Path]) -> Path:
        """
        Save intermediate processing results to JSON file.
        
        Args:
            data: Data to save
            filename: Name of the file
            output_dir: Output directory
            
        Returns:
            Path to the saved file
        """
        import json
        from datetime import datetime
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_path = output_path / f"{filename}_{timestamp}.json"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False, default=str)
        
        logger.debug(f"Saved intermediate result: {file_path}")
        return file_path
    
    @staticmethod
    def load_intermediate_result(file_path: Union[str, Path]) -> Dict:
        """
        Load intermediate processing results from JSON file.
        
        Args:
            file_path: Path to the JSON file
            
        Returns:
            Loaded data dictionary
        """
        import json
        
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"Intermediate result file not found: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.debug(f"Loaded intermediate result: {file_path}")
        return data
    
    @staticmethod
    def get_file_info(file_path: Union[str, Path]) -> Dict[str, Union[str, int, float]]:
        """
        Get comprehensive file information.
        
        Args:
            file_path: Path to the file
            
        Returns:
            Dictionary with file information
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        stat = file_path.stat()
        
        return {
            'name': file_path.name,
            'path': str(file_path),
            'size_bytes': stat.st_size,
            'size_mb': round(stat.st_size / (1024 * 1024), 2),
            'modified_time': stat.st_mtime,
            'created_time': stat.st_ctime,
            'extension': file_path.suffix.lower(),
            'is_file': file_path.is_file(),
            'is_directory': file_path.is_dir(),
        }
    
    @staticmethod
    def validate_file_format(file_path: Union[str, Path]) -> bool:
        """
        Validate if a file is in a supported format.
        
        Args:
            file_path: Path to the file
            
        Returns:
            True if file format is supported, False otherwise
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            return False
        
        # Check file extension
        if file_path.suffix.lower() not in FileUtils.SUPPORTED_EXTENSIONS:
            return False
        
        # Check file size (reasonable limits)
        file_size = file_path.stat().st_size
        if file_size == 0:
            logger.warning(f"File is empty: {file_path}")
            return False
        
        if file_size > 50 * 1024 * 1024:  # 50MB limit
            logger.warning(f"File is too large: {file_path} ({file_size / (1024*1024):.1f}MB)")
            return False
        
        return True 